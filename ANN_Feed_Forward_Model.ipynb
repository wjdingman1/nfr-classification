{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database design    350\n",
      "access control     350\n",
      "security           302\n",
      "privacy            246\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras as keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Get the post processed Data\n",
    "filename = 'DataFiles/PostProcessing.txt'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Show the balance of the dataset\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sentences: (1248, 100)\n",
      "Shape of classes: (1248, 4)\n"
     ]
    }
   ],
   "source": [
    "# Going to get the data first then split it\n",
    "\n",
    "\n",
    "# Dict of words will truncate anything over 2500\n",
    "max_words = 2500\n",
    "\n",
    "# Only allow sentences with less than 100 words\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "\n",
    "# Build the tokenizer for all words in sentences\n",
    "tokenizer.fit_on_texts(df['sentence'].values)\n",
    "\n",
    "# Convert sentences to sequences rather than matrices\n",
    "sentences = tokenizer.texts_to_sequences(df['sentence'].values)\n",
    "\n",
    "# Pad sentences within the max_length\n",
    "sentences = pad_sequences(sentences, maxlen=max_length)\n",
    "print (\"Shape of sentences:\", sentences.shape)\n",
    "\n",
    "# Pull out the classes \n",
    "classes = pd.get_dummies(df['class'].values)\n",
    "print (\"Shape of classes:\", classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1123, 100) (1123, 4)\n",
      "(125, 100) (125, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split testing and training data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(sentences,classes, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1010 samples, validate on 113 samples\n",
      "Epoch 1/20\n",
      "1010/1010 [==============================] - 0s 355us/step - loss: 116.2478 - accuracy: 0.2901 - val_loss: 54.6259 - val_accuracy: 0.3186\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.31858, saving model to best_ANN_model.h5\n",
      "Epoch 2/20\n",
      "1010/1010 [==============================] - 0s 136us/step - loss: 84.4562 - accuracy: 0.3208 - val_loss: 48.4927 - val_accuracy: 0.3451\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.31858 to 0.34513, saving model to best_ANN_model.h5\n",
      "Epoch 3/20\n",
      "1010/1010 [==============================] - 0s 145us/step - loss: 63.8405 - accuracy: 0.3455 - val_loss: 42.7308 - val_accuracy: 0.3540\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.34513 to 0.35398, saving model to best_ANN_model.h5\n",
      "Epoch 4/20\n",
      "1010/1010 [==============================] - 0s 143us/step - loss: 47.2449 - accuracy: 0.3802 - val_loss: 39.9263 - val_accuracy: 0.3540\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.35398\n",
      "Epoch 5/20\n",
      "1010/1010 [==============================] - 0s 137us/step - loss: 42.4571 - accuracy: 0.4099 - val_loss: 40.5286 - val_accuracy: 0.3186\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.35398\n",
      "Epoch 6/20\n",
      "1010/1010 [==============================] - 0s 339us/step - loss: 37.5737 - accuracy: 0.4000 - val_loss: 43.7528 - val_accuracy: 0.3274\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.35398\n",
      "Epoch 7/20\n",
      "1010/1010 [==============================] - 0s 251us/step - loss: 31.7724 - accuracy: 0.4238 - val_loss: 33.9301 - val_accuracy: 0.3097\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.35398\n",
      "Epoch 8/20\n",
      "1010/1010 [==============================] - 0s 303us/step - loss: 24.4011 - accuracy: 0.4535 - val_loss: 36.5936 - val_accuracy: 0.2832\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.35398\n",
      "Epoch 9/20\n",
      "1010/1010 [==============================] - 0s 406us/step - loss: 20.0718 - accuracy: 0.4743 - val_loss: 36.0283 - val_accuracy: 0.3097\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.35398\n",
      "Epoch 10/20\n",
      "1010/1010 [==============================] - 0s 179us/step - loss: 19.8040 - accuracy: 0.4535 - val_loss: 30.5880 - val_accuracy: 0.3097\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.35398\n",
      "Epoch 11/20\n",
      "1010/1010 [==============================] - 0s 146us/step - loss: 15.2119 - accuracy: 0.4723 - val_loss: 30.3084 - val_accuracy: 0.2743\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.35398\n",
      "Epoch 12/20\n",
      "1010/1010 [==============================] - 0s 144us/step - loss: 16.6358 - accuracy: 0.4683 - val_loss: 30.3289 - val_accuracy: 0.2920\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.35398\n",
      "Epoch 13/20\n",
      "1010/1010 [==============================] - 0s 236us/step - loss: 12.2664 - accuracy: 0.5050 - val_loss: 27.4618 - val_accuracy: 0.2743\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.35398\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters that can be tuned\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# List of callbacks to add to the model\n",
    "#\n",
    "# Early stopping will stop training the model if it begins to overfit\n",
    "#\n",
    "# Checkpoint will save the best model from the current training session - based on the highest validation accuracy\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, min_delta=0.001, mode='max')\n",
    "checkpoint = ModelCheckpoint('best_ANN_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callback_list = [early_stopping, checkpoint]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(sentences.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 258us/step\n",
      "Test accuracy: 0.328000009059906\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 354us/step\n",
      "Best test accuracy from training session based on val_accuracy: 0.328000009059906\n"
     ]
    }
   ],
   "source": [
    "# This is the best accuracy of the model during the current training session\n",
    "#\n",
    "# If we get a new best from ALL training session ---> Copy best_ANN_model.h5 and rename it to best_overallANN_model.h5\n",
    "#\n",
    "# Need to rebuild a model instance\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(sentences.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# Load the best weights that were saved from the training session\n",
    "model.load_weights(\"best_ANN_model.h5\")\n",
    "\n",
    "# Compile the new model instance\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Best test accuracy from training session based on val_accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
