{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access control     350\n",
      "database design    350\n",
      "security           302\n",
      "privacy            246\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras as keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, SpatialDropout1D, LSTM\n",
    "\n",
    "\n",
    "# Get the post processed Data\n",
    "filename = 'DataFiles/PostProcessing.txt'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Show the balance of the dataset\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sentences: (1248, 100)\n",
      "Shape of classes: (1248, 4)\n"
     ]
    }
   ],
   "source": [
    "# Going to get the data first then split it\n",
    "# LSTM needs all sentences to tokenize\n",
    "\n",
    "# Dict of words will truncate anything over 2500\n",
    "max_words = 2000\n",
    "\n",
    "# Only allow sentences with less than 100 words\n",
    "max_length = 100\n",
    "\n",
    "# This is fixed...best practice for LSTM\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "\n",
    "# Build the tokenizer for all words in sentences\n",
    "tokenizer.fit_on_texts(df['sentence'].values)\n",
    "\n",
    "# Convert sentences to sequences rather than matrices\n",
    "sentences = tokenizer.texts_to_sequences(df['sentence'].values)\n",
    "\n",
    "# Pad sentences within the max_length\n",
    "sentences = pad_sequences(sentences, maxlen=max_length)\n",
    "print (\"Shape of sentences:\", sentences.shape)\n",
    "\n",
    "# Pull out the classes \n",
    "classes = pd.get_dummies(df['class'].values)\n",
    "print (\"Shape of classes:\", classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1123, 100) (1123, 4)\n",
      "(125, 100) (125, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sentences,classes, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size is \n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "# List of callbacks to add to the model\n",
    "#\n",
    "# Early stopping will stop training the model if it begins to overfit\n",
    "#\n",
    "# Checkpoint will save the best model from the current training session - based on the highest validation accuracy\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, min_delta=0.001, mode='max')\n",
    "checkpoint = ModelCheckpoint('best_RNN_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callback_list = [early_stopping, checkpoint]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=sentences.shape[1]))\n",
    "#model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wes/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1010 samples, validate on 113 samples\n",
      "Epoch 1/30\n",
      "1010/1010 [==============================] - 6s 6ms/step - loss: 1.3433 - accuracy: 0.3545 - val_loss: 1.2961 - val_accuracy: 0.5310\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53097, saving model to best_RNN_model.h5\n",
      "Epoch 2/30\n",
      "1010/1010 [==============================] - 5s 4ms/step - loss: 1.0833 - accuracy: 0.6139 - val_loss: 0.9049 - val_accuracy: 0.6814\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.53097 to 0.68142, saving model to best_RNN_model.h5\n",
      "Epoch 3/30\n",
      "1010/1010 [==============================] - 5s 4ms/step - loss: 0.6782 - accuracy: 0.7772 - val_loss: 0.6459 - val_accuracy: 0.7168\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.68142 to 0.71681, saving model to best_RNN_model.h5\n",
      "Epoch 4/30\n",
      "1010/1010 [==============================] - 4s 4ms/step - loss: 0.3844 - accuracy: 0.8762 - val_loss: 0.5868 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.71681 to 0.79646, saving model to best_RNN_model.h5\n",
      "Epoch 5/30\n",
      "1010/1010 [==============================] - 4s 4ms/step - loss: 0.2532 - accuracy: 0.9228 - val_loss: 0.5343 - val_accuracy: 0.7788\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.79646\n",
      "Epoch 6/30\n",
      "1010/1010 [==============================] - 5s 5ms/step - loss: 0.2038 - accuracy: 0.9396 - val_loss: 0.5609 - val_accuracy: 0.8319\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.79646 to 0.83186, saving model to best_RNN_model.h5\n",
      "Epoch 7/30\n",
      "1010/1010 [==============================] - 5s 5ms/step - loss: 0.1448 - accuracy: 0.9554 - val_loss: 0.5746 - val_accuracy: 0.8319\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.83186\n",
      "Epoch 8/30\n",
      "1010/1010 [==============================] - 4s 4ms/step - loss: 0.1342 - accuracy: 0.9515 - val_loss: 0.5786 - val_accuracy: 0.8230\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.83186\n",
      "Epoch 9/30\n",
      "1010/1010 [==============================] - 4s 4ms/step - loss: 0.0976 - accuracy: 0.9673 - val_loss: 0.5885 - val_accuracy: 0.8319\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.83186\n",
      "Epoch 10/30\n",
      "1010/1010 [==============================] - 5s 5ms/step - loss: 0.0945 - accuracy: 0.9663 - val_loss: 0.6092 - val_accuracy: 0.8319\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.83186\n",
      "Epoch 11/30\n",
      "1010/1010 [==============================] - 6s 6ms/step - loss: 0.0864 - accuracy: 0.9653 - val_loss: 0.6378 - val_accuracy: 0.8142\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.83186\n",
      "Epoch 12/30\n",
      "1010/1010 [==============================] - 4s 4ms/step - loss: 0.0904 - accuracy: 0.9693 - val_loss: 0.6564 - val_accuracy: 0.8053\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.83186\n",
      "Epoch 13/30\n",
      "1010/1010 [==============================] - 5s 5ms/step - loss: 0.0772 - accuracy: 0.9644 - val_loss: 0.6934 - val_accuracy: 0.8142\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.83186\n",
      "Epoch 14/30\n",
      "1010/1010 [==============================] - 5s 4ms/step - loss: 0.0772 - accuracy: 0.9713 - val_loss: 0.6959 - val_accuracy: 0.8230\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.83186\n",
      "Epoch 15/30\n",
      "1010/1010 [==============================] - 4s 4ms/step - loss: 0.0774 - accuracy: 0.9663 - val_loss: 0.7386 - val_accuracy: 0.8053\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.83186\n",
      "Epoch 16/30\n",
      "1010/1010 [==============================] - 5s 4ms/step - loss: 0.0676 - accuracy: 0.9673 - val_loss: 0.7802 - val_accuracy: 0.8053\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.83186\n"
     ]
    }
   ],
   "source": [
    "# Added callbacks from the callback_list\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=callback_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 1ms/step\n",
      "Test accuracy at completion of training session: 0.7839999794960022\n"
     ]
    }
   ],
   "source": [
    "# This is the accuracy of the model at the end of the last epoch during the training session\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy at completion of training session:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 2ms/step\n",
      "Best test accuracy from training session: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "# This is the best accuracy of the model during the current training session\n",
    "#\n",
    "# If we get a new best from ALL training session ---> Copy best_model.h5 and rename it to best_overall_model.h5\n",
    "#\n",
    "# Need to rebuild a model instance\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=sentences.shape[1]))\n",
    "#model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Load the best weights that were saved from the training session\n",
    "model.load_weights(\"best_RNN_model.h5\")\n",
    "\n",
    "# Compile the new model instance\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Best test accuracy from training session:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
